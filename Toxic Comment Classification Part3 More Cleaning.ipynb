{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "## Part3: More Cleaning\n",
    "\n",
    "In Part1, I showed Tfidf + Logistic regression can make good predictions for toxic comment classifications, but found some bad words hidden in disguised words, which had zero coefficients in the logistic regression model. In Part2, I tried more complex models with GloVe word embeddings and LSTM and also simple ensemble methods. The more complex methods outperformed the logistic regression. In this part (Part3), I attempt to clean up those problematic words found in Part1 and see if the simple logistic regression can result in improved performance.\n",
    "\n",
    "Before cleaning comments more, I will first show increasing the bag of words (max_features) from 10,000 to 20,000 can help and do coefficient analysis for each type of toxicity (Part1 only showed coefficient analysis for 'toxicity' and 'indentity hate' types).\n",
    "\n",
    "1. <a href='#Section1'>TFfidf + Logistic Regression with Increased max_features</a>\n",
    "2. <a href='#Section2'>Coefficient Analysis</a>\n",
    "3. <a href='#Section3'>TFfidf + Logistic Regression after More Cleaning</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('train.csv') #training set\n",
    "test = pd.read_csv('test.csv') #test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comments only (pandas series) \n",
    "train_comment = train['comment_text']\n",
    "test_comment = test['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'Section1'></a>\n",
    "## 1. TFfidf + Logistic Regression with Increased max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming function\n",
    "def text_preprocessing(text):\n",
    "    #remove punctuations\n",
    "    text_string = re.sub(r'[^\\w\\s]','',text)\n",
    "    #split text into words\n",
    "    word_list = text_string.split()\n",
    "    #apply stemmer and combine them again\n",
    "    stemmer= SnowballStemmer(\"english\")\n",
    "    words = \"\"\n",
    "    for word in word_list:\n",
    "        if word != \"\":\n",
    "            word_stemmed = stemmer.stem(word)\n",
    "            words += word_stemmed + \" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check original comments in the training set\n",
    "train_comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# stemming all comments in the training set\n",
    "train_comment_stemmed = train_comment.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explan whi the edit made under my usernam hard...\n",
       "1    daww he match this background colour im seem s...\n",
       "2    hey man im realli not tri to edit war it just ...\n",
       "3    more i cant make ani real suggest on improv i ...\n",
       "4    you sir are my hero ani chanc you rememb what ...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comment_stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# stemming all comments in the test set\n",
    "test_comment_stemmed = test_comment.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    yo bitch ja rule is more succes then youll eve...\n",
       "1              from rfc the titl is fine as it is imo \n",
       "2                         sourc zaw ashton on lapland \n",
       "3    if you have a look back at the sourc the infor...\n",
       "4                    i dont anonym edit articl at all \n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comment_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer with max_features 20000\n",
    "A vectorizer converts a collection of text documents into a numerical (sparse) matrix with 1 row per document and 1 column per token (e.g. word). I will use the Tf-idf (term frequency inverse document frequency) vectorizer.\n",
    "\n",
    "I increase max_features from 10,000 to 20,000, but keep all other hyperparameters found to be the best in Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf vectorizing\n",
    "vectorizer = TfidfVectorizer(analyzer ='word', \n",
    "                             stop_words='english',\n",
    "                             sublinear_tf=True, #term-freq scaling\n",
    "                             strip_accents='unicode', #works generally\n",
    "                             token_pattern=r'\\w{1,}', #1+ char words\n",
    "                             ngram_range=(1,1),\n",
    "                             max_features=20000) #top frequent 20000 words\n",
    "\n",
    "vectorizer.fit(pd.concat([train_comment_stemmed,test_comment_stemmed]))\n",
    "\n",
    "train_feature_matrix = vectorizer.transform(train_comment_stemmed)\n",
    "test_feature_matrix = vectorizer.transform(test_comment_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for checking results at glance\n",
    "def print_results():\n",
    "    print(\"### {} ###\".format(category))\n",
    "    print()\n",
    "    print(\"Best hyper-parameters on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid search scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.4f (+/-%0.04f) for %r\" % (mean, std * 2, params))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.columns # just to coveniently copy and paste category names below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =['toxic', 'severe_toxic', 'obscene', \n",
    "             'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9639 (+/-0.0042) for {'C': 0.2}\n",
      "0.9683 (+/-0.0033) for {'C': 0.5}\n",
      "0.9701 (+/-0.0027) for {'C': 1}\n",
      "0.9707 (+/-0.0023) for {'C': 2}\n",
      "0.9695 (+/-0.0019) for {'C': 5}\n",
      "\n",
      "### severe_toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 0.5}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9844 (+/-0.0025) for {'C': 0.2}\n",
      "0.9851 (+/-0.0028) for {'C': 0.5}\n",
      "0.9850 (+/-0.0032) for {'C': 1}\n",
      "0.9844 (+/-0.0037) for {'C': 2}\n",
      "0.9824 (+/-0.0043) for {'C': 5}\n",
      "\n",
      "### obscene ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9818 (+/-0.0038) for {'C': 0.2}\n",
      "0.9839 (+/-0.0036) for {'C': 0.5}\n",
      "0.9848 (+/-0.0035) for {'C': 1}\n",
      "0.9849 (+/-0.0034) for {'C': 2}\n",
      "0.9836 (+/-0.0032) for {'C': 5}\n",
      "\n",
      "### threat ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9767 (+/-0.0026) for {'C': 0.2}\n",
      "0.9802 (+/-0.0029) for {'C': 0.5}\n",
      "0.9825 (+/-0.0038) for {'C': 1}\n",
      "0.9836 (+/-0.0046) for {'C': 2}\n",
      "0.9827 (+/-0.0057) for {'C': 5}\n",
      "\n",
      "### insult ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9732 (+/-0.0014) for {'C': 0.2}\n",
      "0.9754 (+/-0.0010) for {'C': 0.5}\n",
      "0.9762 (+/-0.0008) for {'C': 1}\n",
      "0.9759 (+/-0.0008) for {'C': 2}\n",
      "0.9736 (+/-0.0007) for {'C': 5}\n",
      "\n",
      "### identity_hate ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9717 (+/-0.0033) for {'C': 0.2}\n",
      "0.9743 (+/-0.0029) for {'C': 0.5}\n",
      "0.9752 (+/-0.0027) for {'C': 1}\n",
      "0.9752 (+/-0.0027) for {'C': 2}\n",
      "0.9733 (+/-0.0026) for {'C': 5}\n",
      "\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    parameters = {'C': [.2, .5, 1, 2, 5]}\n",
    "    log_reg = LogisticRegression(solver = 'sag',random_state =42)   \n",
    "    clf= GridSearchCV(log_reg, parameters, scoring='roc_auc', cv=3)\n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.9}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9710 (+/-0.0014) for {'C': 1.1}\n",
      "0.9711 (+/-0.0013) for {'C': 1.2}\n",
      "0.9711 (+/-0.0013) for {'C': 1.3}\n",
      "0.9712 (+/-0.0013) for {'C': 1.4}\n",
      "0.9712 (+/-0.0012) for {'C': 1.5}\n",
      "0.9713 (+/-0.0012) for {'C': 1.6}\n",
      "0.9713 (+/-0.0012) for {'C': 1.7}\n",
      "0.9713 (+/-0.0012) for {'C': 1.8}\n",
      "0.9713 (+/-0.0011) for {'C': 1.9}\n",
      "0.9713 (+/-0.0011) for {'C': 2.0}\n",
      "0.9713 (+/-0.0011) for {'C': 2.1}\n",
      "0.9713 (+/-0.0011) for {'C': 2.2}\n",
      "0.9712 (+/-0.0011) for {'C': 2.3}\n",
      "0.9712 (+/-0.0010) for {'C': 2.4}\n",
      "0.9712 (+/-0.0010) for {'C': 2.5}\n",
      "\n",
      "### severe_toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 0.6}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9841 (+/-0.0028) for {'C': 0.1}\n",
      "0.9848 (+/-0.0028) for {'C': 0.2}\n",
      "0.9852 (+/-0.0029) for {'C': 0.3}\n",
      "0.9853 (+/-0.0030) for {'C': 0.4}\n",
      "0.9854 (+/-0.0032) for {'C': 0.5}\n",
      "0.9854 (+/-0.0033) for {'C': 0.6}\n",
      "0.9854 (+/-0.0035) for {'C': 0.7}\n",
      "0.9854 (+/-0.0036) for {'C': 0.8}\n",
      "0.9854 (+/-0.0037) for {'C': 0.9}\n",
      "0.9853 (+/-0.0038) for {'C': 1.0}\n",
      "\n",
      "### obscene ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.5}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9854 (+/-0.0033) for {'C': 1.1}\n",
      "0.9855 (+/-0.0033) for {'C': 1.2}\n",
      "0.9855 (+/-0.0033) for {'C': 1.3}\n",
      "0.9855 (+/-0.0033) for {'C': 1.4}\n",
      "0.9855 (+/-0.0033) for {'C': 1.5}\n",
      "0.9855 (+/-0.0033) for {'C': 1.6}\n",
      "0.9855 (+/-0.0033) for {'C': 1.7}\n",
      "0.9855 (+/-0.0033) for {'C': 1.8}\n",
      "0.9855 (+/-0.0033) for {'C': 1.9}\n",
      "0.9855 (+/-0.0033) for {'C': 2.0}\n",
      "0.9854 (+/-0.0033) for {'C': 2.1}\n",
      "0.9854 (+/-0.0033) for {'C': 2.2}\n",
      "0.9854 (+/-0.0033) for {'C': 2.3}\n",
      "0.9853 (+/-0.0033) for {'C': 2.4}\n",
      "0.9853 (+/-0.0033) for {'C': 2.5}\n",
      "\n",
      "### threat ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.9}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9832 (+/-0.0080) for {'C': 1.1}\n",
      "0.9833 (+/-0.0080) for {'C': 1.2}\n",
      "0.9834 (+/-0.0080) for {'C': 1.3}\n",
      "0.9835 (+/-0.0080) for {'C': 1.4}\n",
      "0.9836 (+/-0.0080) for {'C': 1.5}\n",
      "0.9836 (+/-0.0080) for {'C': 1.6}\n",
      "0.9837 (+/-0.0081) for {'C': 1.7}\n",
      "0.9837 (+/-0.0081) for {'C': 1.8}\n",
      "0.9837 (+/-0.0081) for {'C': 1.9}\n",
      "0.9837 (+/-0.0082) for {'C': 2.0}\n",
      "0.9837 (+/-0.0082) for {'C': 2.1}\n",
      "0.9837 (+/-0.0082) for {'C': 2.2}\n",
      "0.9837 (+/-0.0082) for {'C': 2.3}\n",
      "0.9836 (+/-0.0083) for {'C': 2.4}\n",
      "0.9836 (+/-0.0083) for {'C': 2.5}\n",
      "\n",
      "### insult ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.1}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9763 (+/-0.0027) for {'C': 0.6}\n",
      "0.9765 (+/-0.0027) for {'C': 0.7}\n",
      "0.9766 (+/-0.0028) for {'C': 0.8}\n",
      "0.9767 (+/-0.0028) for {'C': 0.9}\n",
      "0.9767 (+/-0.0029) for {'C': 1.0}\n",
      "0.9767 (+/-0.0029) for {'C': 1.1}\n",
      "0.9767 (+/-0.0030) for {'C': 1.2}\n",
      "0.9767 (+/-0.0030) for {'C': 1.3}\n",
      "0.9767 (+/-0.0030) for {'C': 1.4}\n",
      "0.9767 (+/-0.0031) for {'C': 1.5}\n",
      "\n",
      "### identity_hate ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9752 (+/-0.0060) for {'C': 0.6}\n",
      "0.9754 (+/-0.0060) for {'C': 0.7}\n",
      "0.9755 (+/-0.0061) for {'C': 0.8}\n",
      "0.9756 (+/-0.0061) for {'C': 0.9}\n",
      "0.9757 (+/-0.0061) for {'C': 1.0}\n",
      "0.9757 (+/-0.0061) for {'C': 1.1}\n",
      "0.9757 (+/-0.0062) for {'C': 1.2}\n",
      "0.9757 (+/-0.0062) for {'C': 1.3}\n",
      "0.9757 (+/-0.0062) for {'C': 1.4}\n",
      "0.9757 (+/-0.0062) for {'C': 1.5}\n",
      "\n",
      "Wall time: 10min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune C for each category CV=5 \n",
    "parameters_dic={'toxic':{'C':[1.1,1.2,1.3,1.4,1.5, 1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5]},\n",
    "                'severe_toxic':{'C':[.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0]},\n",
    "                'obscene':{'C':[1.1,1.2,1.3,1.4,1.5, 1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5]},\n",
    "                'threat':{'C':[1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5]}, \n",
    "                'insult':{'C':[.6,.7,.8,.9,1.0,1.1,1.2,1.3,1.4,1.5]}, \n",
    "                'identity_hate':{'C':[.6,.7,.8,.9,1.0,1.1,1.2,1.3,1.4,1.5]}}\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    parameters = parameters_dic[category]\n",
    "    log_reg = LogisticRegression(solver='sag', random_state =42)   \n",
    "    clf= GridSearchCV(log_reg, parameters, scoring='roc_auc', cv=5)\n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for the inverse regularization parameter C with 20000 for max_feature are slightly different from those with 10000 for  max_feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score on dev. set for toxic is 0.9717\n",
      "Mean CV score on dev. set for severe_toxic is 0.9857\n",
      "Mean CV score on dev. set for obscene is 0.9857\n",
      "Mean CV score on dev. set for threat is 0.9842\n",
      "Mean CV score on dev. set for insult is 0.9770\n",
      "Mean CV score on dev. set for identity_hate is 0.9767\n",
      "\n",
      "Mean CV score on dev. set for All categories is 0.9802\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_C = {'toxic':1.9,'severe_toxic':.6,'obscene':1.5,\n",
    "        'threat':1.9, 'insult':1.1, 'identity_hate':1.2}\n",
    "score_all = []\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    log_reg = LogisticRegression(solver='sag',random_state =42)   \n",
    "    clf= GridSearchCV(log_reg, {'C':[best_C[category]]}, scoring='roc_auc', cv=10)\n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    print(\"Mean CV score on dev. set for %s is %0.4f\" %(category,clf.best_score_))\n",
    "    score_all.append(clf.best_score_)\n",
    "print()\n",
    "print(\"Mean CV score on dev. set for All categories is %0.4f\" %np.mean(score_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20000 for max_feature gives slightly better performance than 10000 (0.9802 vs. 0.9799); 20000 is better for all types of toxicity except for Threat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    clf = LogisticRegression(solver='sag', C=best_C[category],random_state =42)   \n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    submission[category] = clf.predict_proba(test_feature_matrix)[:,1] #second column!!\n",
    "    \n",
    "submission.to_csv('submission_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only method to test my model with the testset is submitting predictions to the leaderboard (LB) in Kaggle. I found AUC for the testset is .9747 on the leaderboard (Part1 LB AUC: 0.9745). Doubling max_features increased AUC by 0.0002 (0.78% decrease in error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7843137254901107"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error change in percent\n",
    "(0.9747-0.9745)/(1-0.9745)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'Section2'></a>\n",
    "## 2. Coefficient Analysis\n",
    "\n",
    "Here I check words with high, low (negative) and zero coefficients to check what are most and least important words in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of word:feature_index key value pairs\n",
    "voca_dic = vectorizer.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of words odered by feature index\n",
    "feature_words =[None]*len(voca_dic)\n",
    "for word, index in voca_dic.items():\n",
    "    feature_words[index]=word\n",
    "#print(feature_words[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category: type of toxicity / num_to_check: number of words to output to check\n",
    "def coeff_analysis(category, num_to_check):\n",
    "    clf = LogisticRegression(solver='sag', C=best_C[category],random_state =42)   \n",
    "    clf.fit(train_feature_matrix, train[category])\n",
    "    coeff = clf.coef_\n",
    "    #print(\"coefficients:\\n\", coeff, \"\\n\")\n",
    "\n",
    "    # High coefficient words\n",
    "    sorted_voca = [word for _,word in sorted(zip(coeff[0],feature_words),reverse=True)]\n",
    "    print(\"High coefficient words:\\n\", sorted_voca[:num_to_check])\n",
    "    print()\n",
    "    # Low coefficient words (negative coefficients)\n",
    "    sorted_voca = [word for _,word in sorted(zip(coeff[0],feature_words))]\n",
    "    print(\"Low (negative) coefficient words:\\n\", sorted_voca[:num_to_check])\n",
    "    print()\n",
    "    # Words with low absolute values of coefficient (just out of curiosity)\n",
    "    sorted_voca = [word for _,word in sorted(zip(list(map(abs,coeff[0])),feature_words))]\n",
    "    #sorted_voca = [(coef,word) for coef,word in sorted(zip(list(map(abs,coeff[0])),feature_words))]\n",
    "    print(\"Close to zero or zero coefficient words:\\n\", sorted_voca[:num_to_check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['fuck', 'idiot', 'stupid', 'shit', 'suck', 'bullshit', 'ass', 'asshol', 'bitch', 'dick', 'crap', 'faggot', 'peni', 'bastard', 'moron', 'pathet', 'cunt', 'nigger', 'jerk', 'fucker', 'shut', 'gay', 'dumbass', 'cock', 'hell', 'motherfuck', 'loser', 'retard', 'dumb', 'fag', 'pussi', 'liar', 'fuckin', 'racist', 'piss', 'fool', 'nazi', 'kill', 'dickhead', 'sex', 'jackass', 'hate', 'wtf', 'sick', 'damn', 'cocksuck', 'pig', 'whore', 'homosexu', 'die', 'bloodi', 'garbag', 'douchebag', 'scum', 'f', 'hypocrit', 'goddamn', 'wanker', 'fing', 'ars', 'vagina', 'fat', 'fk', 'dirti', 'disgust', 'fuckhead', 'nerd', 'fck', 'prick', 'dipshit', 'pedophil', 'fascist', 'anal', 'fart', 'ugli', 'screw', 'freak', 'rubbish', 'rape', 'donkey', 'shame', 'ridicul', 'porn', 'coward', 'fcking', 'imbecil', 'stink', 'nigga', 'monkey', 'fuk', 'lick', 'homo', 'mental', 'shitti', 'ahol', 'butt', 'mouth', 'looser', 'foolish', 'fuckwit']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['thank', 'redirect', 'pleas', 'appreci', 'best', 'cheer', 'mention', 'continu', 'case', 'section', 'sorri', 'agre', 'titl', 'apolog', 'articl', 'consensus', 'request', 'lead', 'use', '2004', 'sourc', 'list', 'merg', 'experi', 'sure', 'present', 'michael', 'earli', 'consid', 'reach', 'week', 'laurent', 'wikiproject', 'themselv', 'season', 'futur', 'review', 'promis', 'exampl', '2005', 'refer', 'point', 'record', 'paragraph', 'type', 'issu', 'descript', 'book', 'connect', 'februari', 'involv', 'popular', 'young', 'howev', 'assist', 'process', 'email', 'elimin', 'latin', 'contrib', 'disput', 'publish', 'propos', 'number', 'updat', 'talk', 'januari', 'accord', 'fought', 'movement', 'welcom', 'notic', 'includ', 'idea', 'ive', 'identifi', 'expand', 'good', 'rfc', 'movi', 'question', 'similar', 'link', 'believ', 'definit', 'mistak', 'convinc', 'influenc', 'help', 'rang', 'templat', 'context', 'result', 'specif', 'mail', 'smith', 'long', 'april', 'relat', 'attribut']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['111111111111111111111111oneoneoneonoeneryan', '19025427492', '1982que', '439enter', '5h1t', '5hut', '5uck5', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaafuck', 'aad', 'aahahahahahahjaahahahahahahaahh', 'abstimmungsgegn', 'abujihad', 'adalah', 'adam1213', 'adsydfiusagjfasfsduyaidfasgiudf', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'analan', 'anarchistfuck', 'anderen', 'answershi', 'apparition11', 'arrs', 'arrsephuck', 'asdf', 'asodfejvwosimwcrsdsfdhuicfjsmsdiovuhdiyhcsdiuvnhscdifuhmhelllo', 'asspi', 'atheismus', 'aur', 'ayden', 'aysen', 'azzholeseat', 'b00ll00x', 'backgroundcolorlavend', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'ballshut', 'barbercallum', 'basmati', 'bastardsalso', 'bastardshel', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitchaww', 'bitchbot', 'bitchhitl', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'buttslut', 'bytyn', 'c0pyr1ght', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'clustercuntfartymcflyupmycuntholebastardfuck', 'cockebaum', 'cockjeremi', 'cockroacha', 'comnuistsscrew', 'congeal', 'coonhahaahahahahahahaha', 'cowboysgay', 'cua', 'culet', 'cumfil', 'cuntass', 'cuntnlu', 'cοck', 'd0wn']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('toxic',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Severe Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['fuck', 'motherfuck', 'suck', 'bitch', 'asshol', 'fuckin', 'dick', 'fucker', 'shit', 'ass', 'faggot', 'die', 'cunt', 'rape', 'cock', 'cocksuck', 'nigger', 'bastard', 'kill', 'gay', 'fcking', 'fag', 'u', 'fat', 'peni', 'f', 'dumb', 'nazi', 'pig', 'stupid', 'shut', 'prick', 'whore', 'dirti', 'moron', 'piec', 'fing', 'fuke', 'filthi', 'fuk', 'mother', 'head', 'jew', 'cancer', 'mothjer', 'fucken', 'fck', 'dickhead', 'son', 'burn', 'big', 'sucker', 'retard', 'proassadhanibal911your', 'gonna', 'nerd', 'mouth', 'youfuck', 'slut', 'hell', 'pussi', 'dare', 'hate', 'butt', 'nigga', 'wanker', 'anal', 'homosexu', 'mom', 'ars', 'n', 'hairi', 'lick', 'douch', 'cum', 'rot', 'ur', 'shoot', 'urself', 'wank', 'pathet', 'splatter', 'bich', 'piss', 'loser', 'ya', 'eat', 'murder', 'fcuk', 'abus', 'damn', 'stick', 'georg', 'sex', 'dik', 'homo', 'fukin', 'ugli', 'hey', 'fuckfac']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['pleas', 'articl', 'talk', 'thank', 'use', 'sourc', 'link', 'fact', 'sure', 'redirect', 'list', 'ani', 'utc', 'need', 'creat', 'think', 'agre', 'refer', 'look', 'section', 'discuss', 'right', 'state', 'ask', 'edit', 'veri', 'comment', 'help', 'remov', 'chang', 'word', 'question', 'actual', 'onli', 'suggest', 'read', 'becaus', 'doe', 'person', 'includ', 'mention', 'did', 'new', 'believ', 'issu', 'mani', 'point', 'ive', 'just', 'welcom', 'reason', 'alreadi', 'clear', 'regard', 'user', 'anyth', 'explain', 'info', 'say', 'ok', 'vandal', 'notic', 'histori', 'note', 'anyon', 'case', 'sorri', 'recent', 'yes', 'ad', 'cours', 'exampl', 'request', 'pretti', 'content', 'certain', 'offici', 'subject', 'perhap', 'hi', 'titl', 'mind', 'origin', 'possibl', 'violat', 'better', 'templat', 'make', 'wonder', 'term', 'probabl', 'song', 'mess', 'friend', 'german', 'dutch', 'howev', 'version', 'mayb', 'polici']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['111111111111111111111111oneoneoneonoeneryan', '19025427492', '1982que', '439enter', '5h1t', '5hut', '5uck5', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaafuck', 'aad', 'aahahahahahahjaahahahahahahaahh', 'abstimmungsgegn', 'abujihad', 'adalah', 'adam1213', 'adsydfiusagjfasfsduyaidfasgiudf', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'analan', 'anarchistfuck', 'anderen', 'answershi', 'apparition11', 'arrs', 'arrsephuck', 'asdf', 'asodfejvwosimwcrsdsfdhuicfjsmsdiovuhdiyhcsdiuvnhscdifuhmhelllo', 'asspi', 'atheismus', 'aur', 'ayden', 'aysen', 'azzholeseat', 'b00ll00x', 'backgroundcolorlavend', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'ballshut', 'barbercallum', 'basmati', 'bastardsalso', 'bastardshel', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitchaww', 'bitchbot', 'bitchhitl', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'buttslut', 'bytyn', 'c0pyr1ght', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'clustercuntfartymcflyupmycuntholebastardfuck', 'cockebaum', 'cockjeremi', 'cockroacha', 'comnuistsscrew', 'congeal', 'coonhahaahahahahahahaha', 'cowboysgay', 'cua', 'culet', 'cumfil', 'cuntass', 'cuntnlu', 'cοck', 'd0wn']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('severe_toxic',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['fuck', 'asshol', 'ass', 'bitch', 'shit', 'bullshit', 'suck', 'dick', 'cunt', 'bastard', 'fucker', 'faggot', 'motherfuck', 'fuckin', 'pussi', 'stupid', 'cock', 'dumbass', 'peni', 'crap', 'idiot', 'damn', 'dickhead', 'cocksuck', 'fcking', 'jackass', 'nigger', 'jerk', 'wtf', 'fag', 'fk', 'fuckhead', 'moron', 'fck', 'anal', 'f', 'scum', 'ars', 'fuk', 'piss', 'whore', 'fuckwit', 'cum', 'butt', 'fucktard', 'slut', 'fing', 'lick', 'dumb', 'ahol', 'anus', 'hell', 'goddamn', 'rape', 'dipshit', 'vagina', 'sex', 'prick', 'fcuk', 'fool', 'homosexu', 'shithead', 'dumbfuck', 'ball', 'tit', 'filthi', 'fucken', 'fuke', 'fat', 'twat', 'scumbag', 'sht', 'nigga', 'masturb', 'blowjob', 'mom', 'dickfac', 'wanker', 'fking', 'shitti', 'shut', 'bloodi', 'u', 'mouth', 'asshat', 'bag', 'douch', 'n', 'piec', 'screw', 'bich', 'dirti', 'slick', 'fu', 'pig', 'boob', 'hole', 'freak', 'fggt', 'blow']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['thank', 'redirect', 'articl', 'sure', 'agre', 'cheer', 'base', 'exampl', 'laurent', 'pleas', 'mention', 'section', 'utc', 'talk', 'note', 'look', 'theori', 'best', 'sourc', 'includ', 'valu', '19', 'suggest', 'propos', 'merg', 'check', 'titl', 'join', 'ad', 'accept', 'book', 'week', 'discuss', 'contrib', 'tag', 'stori', 'war', 'support', 'use', 'welcom', 'ask', 'materi', 'televis', 'list', 'definit', 'notic', 'main', 'nomin', 'refer', 'sorri', 'specif', 'request', 'sign', 'becous', 'known', 'ani', 'offer', 'barnstar', '2014', 'id', 'april', 'accord', 'updat', 'meant', 'ref', 'unit', 'common', 'addit', 'link', 'need', 'men', 'continu', 'concern', 'editor', 'fall', 'offici', 'repres', 'mayb', 'comment', 'version', 'help', 'review', 'crime', 'parti', 'edit', 'yes', 'templat', 'consensus', 'perhap', 'infobox', 'rumor', 'afraid', 'regard', 'believ', 'consid', 'origin', 'dure', 'besid', 'research', 'punish']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['111111111111111111111111oneoneoneonoeneryan', '19025427492', '1982que', '439enter', '5h1t', '5hut', '5uck5', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaafuck', 'aad', 'aahahahahahahjaahahahahahahaahh', 'abstimmungsgegn', 'abujihad', 'adalah', 'adam1213', 'adsydfiusagjfasfsduyaidfasgiudf', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'analan', 'anarchistfuck', 'anderen', 'answershi', 'apparition11', 'arrs', 'arrsephuck', 'asdf', 'asodfejvwosimwcrsdsfdhuicfjsmsdiovuhdiyhcsdiuvnhscdifuhmhelllo', 'asspi', 'atheismus', 'aur', 'ayden', 'aysen', 'azzholeseat', 'b00ll00x', 'backgroundcolorlavend', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'ballshut', 'barbercallum', 'basmati', 'bastardsalso', 'bastardshel', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitchaww', 'bitchbot', 'bitchhitl', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'buttslut', 'bytyn', 'c0pyr1ght', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'clustercuntfartymcflyupmycuntholebastardfuck', 'cockebaum', 'cockjeremi', 'cockroacha', 'comnuistsscrew', 'congeal', 'coonhahaahahahahahahaha', 'cowboysgay', 'cua', 'culet', 'cumfil', 'cuntass', 'cuntnlu', 'cοck', 'd0wn']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('obscene',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['kill', 'die', 'rape', 'death', 'cut', 'burn', 'destroy', 'hunt', 'ill', 'beat', 'shoot', 'hang', 'stab', 'punch', 'hope', 'kick', 'ass', 'dead', 'knock', 'hous', 'face', 'live', 'murder', 'deserv', 'head', 'homosexu', 'hell', 'shot', 'blood', 'slowli', 'im', 'fuck', 'supertr0l', 'gonna', 'corps', 'knife', 'splatter', 'roast', 'disgust', 'watch', 'shall', 'aliv', 'ya', 'life', 'slit', 'skin', 'bloodi', 'gun', 'rvv', 'swear', 'children', 'behead', 'hate', 'smash', 'fking', 'yo', 'jew', 'pain', 'neck', 'castrat', 'fuckin', 'send', 'pull', 'traitor', 'robe', 'miseri', 'horribl', 'hit', 'track', 'come', 'dare', 'throw', 'onc', 'fat', 'hello', 'poop', 'gross', 'fuke', 'shithead', 'pineappl', 'motherfuck', 'bastard', 'throat', 'testicl', 'u', 'll', 'nazi', 'gut', 'blow', 'ki', 'stupid', 'feed', 'heil', 'rv', 'wear', 'reveng', 'wikistalk', '8d', 'laugh', 'suicid']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['thank', 'articl', 'sourc', 'someon', 'pleas', 'chang', 'say', 'doe', 'onli', 'redirect', 'list', 'work', 'section', 'did', 'sure', 'refer', 'use', 'remov', 'editor', 'sinc', 'didnt', 'help', 'discuss', 'add', 'ad', 'link', 'mani', 'actual', 'mention', 'page', 'whatev', 'isnt', 'read', 'cheer', 'think', 'abov', 'histori', 'includ', 'book', 'creat', 'idea', 'care', 'write', 'doesnt', 'ok', 'hes', 'utc', 'vandal', 'case', 'becaus', 'oh', 'ive', 'simpli', 'ask', 'need', 'second', 'lol', 'question', 'pictur', 'administr', 'later', 'note', 'game', 'seen', 'wonder', 'accus', 'got', 'america', 'state', 'agre', 'fight', 'love', 'fix', 'bore', 'origin', 'tri', 'english', 'like', 'lot', 'anoth', 'bit', 'film', 'ani', 'problem', 'peopl', 'explain', 'possibl', 'site', 'tag', 'sorri', 'wrong', 'bias', 'guess', 'talk', 'review', 'shame', 'answer', 'complet', 'comment', 'respons']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['111111111111111111111111oneoneoneonoeneryan', '19025427492', '1982que', '439enter', '5h1t', '5hut', '5uck5', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaafuck', 'aad', 'aahahahahahahjaahahahahahahaahh', 'abstimmungsgegn', 'abujihad', 'adalah', 'adam1213', 'adsydfiusagjfasfsduyaidfasgiudf', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'analan', 'anarchistfuck', 'anderen', 'answershi', 'apparition11', 'arrs', 'arrsephuck', 'asdf', 'asodfejvwosimwcrsdsfdhuicfjsmsdiovuhdiyhcsdiuvnhscdifuhmhelllo', 'asspi', 'atheismus', 'aur', 'ayden', 'aysen', 'azzholeseat', 'b00ll00x', 'backgroundcolorlavend', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'ballshut', 'barbercallum', 'basmati', 'bastardsalso', 'bastardshel', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitchaww', 'bitchbot', 'bitchhitl', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'buttslut', 'bytyn', 'c0pyr1ght', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'clustercuntfartymcflyupmycuntholebastardfuck', 'cockebaum', 'cockjeremi', 'cockroacha', 'comnuistsscrew', 'congeal', 'coonhahaahahahahahahaha', 'cowboysgay', 'cua', 'culet', 'cumfil', 'cuntass', 'cuntnlu', 'cοck', 'd0wn']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('threat',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['idiot', 'stupid', 'asshol', 'bitch', 'fuck', 'faggot', 'bastard', 'moron', 'cunt', 'ass', 'suck', 'dumb', 'jerk', 'fucker', 'dickhead', 'dick', 'retard', 'fool', 'pathet', 'loser', 'dumbass', 'scum', 'shit', 'nigger', 'motherfuck', 'fat', 'pig', 'fuckin', 'cocksuck', 'fag', 'coward', 'jackass', 'nigga', 'liar', 'scumbag', 'gay', 'slut', 'fcking', 'douchebag', 'ugli', 'fuckhead', 'whore', 'nazi', 'homosexu', 'freak', 'mouth', 'dirti', 'douch', 'prick', 'homo', 'ahol', 'maggot', 'nerd', 'imbecil', 'piec', 'mom', 'fuckwit', 'shithead', 'fucktard', 'hypocrit', 'stink', 'racist', 'pussi', 'fcuk', 'crap', 'goddamn', 'bullshit', 'f', 'smell', 'anus', 'filthi', 'wanker', 'cock', 'littl', 'twat', 'peni', 'looser', 'bulli', 'rape', 'shut', 'bag', 'hell', 'fascist', 'fing', 'damn', 'sick', 'anal', 'disgust', 'sucker', 'lazi', 'mother', 'prostitut', 'fggt', 'bloodi', 'fk', 'die', 'head', 'monkey', 'fuk', 'pompous']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['thank', 'redirect', 'articl', 'pleas', 'talk', 'mention', 'section', 'help', 'cheer', 'agre', 'sourc', 'howev', 'statement', 'review', 'utc', 'result', 'refer', 'includ', 'merg', 'specif', 'discuss', 'tag', 'use', 'respons', 'origin', 'sorri', 'nomin', 'exampl', 'link', 'abov', 'claim', 'regard', 'number', 'song', 'wrong', 'version', 'check', 'welcom', 'paragraph', 'support', 'disagre', 'theori', 'general', 'appear', '2008', 'main', 'start', 'april', 'accord', 'request', 'appreci', 'communiti', 'english', 'avoid', 'ad', 'definit', 'suggest', 'unit', 'pass', 'look', 'best', 'sandbox', 'matter', 'list', 'ive', 'unblock', 'citat', 'note', 'went', '2004', 'disput', 'context', 'sure', 'stori', 'categori', 'point', 'propos', 'argument', 'ago', 'ok', 'titl', 'coupl', 'sentenc', '19', 'email', 'format', 'anyon', 'continu', 'barnstar', 'wikiproject', 'thread', 'reflect', 'phrase', 'ani', 'recent', 'awar', 'copi', 'test', 'share', 'error']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['111111111111111111111111oneoneoneonoeneryan', '19025427492', '1982que', '439enter', '5h1t', '5hut', '5uck5', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaafuck', 'aad', 'aahahahahahahjaahahahahahahaahh', 'abstimmungsgegn', 'abujihad', 'adalah', 'adam1213', 'adsydfiusagjfasfsduyaidfasgiudf', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'analan', 'anarchistfuck', 'anderen', 'answershi', 'apparition11', 'arrs', 'arrsephuck', 'asdf', 'asodfejvwosimwcrsdsfdhuicfjsmsdiovuhdiyhcsdiuvnhscdifuhmhelllo', 'asspi', 'atheismus', 'aur', 'ayden', 'aysen', 'azzholeseat', 'b00ll00x', 'backgroundcolorlavend', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'ballshut', 'barbercallum', 'basmati', 'bastardsalso', 'bastardshel', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitchaww', 'bitchbot', 'bitchhitl', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'buttslut', 'bytyn', 'c0pyr1ght', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'clustercuntfartymcflyupmycuntholebastardfuck', 'cockebaum', 'cockjeremi', 'cockroacha', 'comnuistsscrew', 'congeal', 'coonhahaahahahahahahaha', 'cowboysgay', 'cua', 'culet', 'cumfil', 'cuntass', 'cuntnlu', 'cοck', 'd0wn']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('insult',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['nigger', 'gay', 'nigga', 'homosexu', 'jew', 'faggot', 'nazi', 'homo', 'muslim', 'racist', 'black', 'negro', 'fag', 'white', 'asian', 'hate', 'american', 'scum', 'rape', 'fuck', 'lesbian', 'indian', 'turk', 'arab', 'stupid', 'homophob', 'dirti', 'fucker', 'nig', 'monkey', 'mexican', 'fat', 'niger', 'jewish', 'pedophil', 'shit', 'terrorist', 'niggaz', 'twat', 'chink', 'fagot', 'women', 'sucker', 'dumb', 'fool', 'burn', 'retard', 'disgust', 'boy', 'russian', 'r', 'allah', 'bitch', 'bastard', 'littl', 'queer', 'cunt', 'slave', 'damn', 'communist', 'whyd', 'fuckin', 'pig', 'slut', 'antisemit', 'like', 'irish', 'chines', 'g', 'death', 'islam', 'akbar', 'babi', 'jewhat', 'countri', 'trash', 'jesus', 'whore', 'ass', 'redneck', 'slavic', 'filthi', 'evil', 'hoe', 'filth', 'nl33er', 'kill', 'hitler', 'bag', 'dipshit', 'nao', 'anal', 'u', 'closet', 'malaysian', 'peni', 'motherfuck', 'crazi', 'idiot', 'huge']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['articl', 'talk', 'thank', 'sourc', 'agre', 'redirect', 'mention', 'editor', 'ani', 'origin', 'better', 'discuss', 'consid', 'read', 'clear', 'issu', 'person', 'list', 'note', 'import', 'look', 'ok', 'actual', 'pleas', 'link', 'understand', 'way', 'differ', 'suggest', 'follow', 'word', 'polici', 'alon', 'book', 'remov', 'befor', 'happen', 'problem', 'mean', 'chang', 'help', 'isnt', 'sinc', 'inform', 'cheer', 'definit', 'new', 'say', 'marriag', 'case', 'time', 'check', 'ive', 'worthless', 'state', 'edit', 'open', 'end', 'id', 'utc', 'exampl', 'histori', 'week', '2008', 'rot', 'use', 'accus', 'notabl', 'tag', 'includ', 'review', 'number', 'refer', 'seen', 'languag', 'didnt', 'answer', 'result', 'page', 'fix', 'critic', 'possibl', 'someon', 'simpli', 'sure', 'posit', 'version', 'subject', 'line', 'howev', 'bit', 'term', 'provid', 'did', 'certain', 'deni', 'present', 'month', 'imag', 'mani']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['111111111111111111111111oneoneoneonoeneryan', '19025427492', '1982que', '439enter', '5h1t', '5hut', '5uck5', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaafuck', 'aad', 'aahahahahahahjaahahahahahahaahh', 'abstimmungsgegn', 'abujihad', 'adalah', 'adam1213', 'adsydfiusagjfasfsduyaidfasgiudf', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'analan', 'anarchistfuck', 'anderen', 'answershi', 'apparition11', 'arrs', 'arrsephuck', 'asdf', 'asodfejvwosimwcrsdsfdhuicfjsmsdiovuhdiyhcsdiuvnhscdifuhmhelllo', 'asspi', 'atheismus', 'aur', 'ayden', 'aysen', 'azzholeseat', 'b00ll00x', 'backgroundcolorlavend', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'ballshut', 'barbercallum', 'basmati', 'bastardsalso', 'bastardshel', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitchaww', 'bitchbot', 'bitchhitl', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'buttslut', 'bytyn', 'c0pyr1ght', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'clustercuntfartymcflyupmycuntholebastardfuck', 'cockebaum', 'cockjeremi', 'cockroacha', 'comnuistsscrew', 'congeal', 'coonhahaahahahahahahaha', 'cowboysgay', 'cua', 'culet', 'cumfil', 'cuntass', 'cuntnlu', 'cοck', 'd0wn']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('identity_hate',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high coefficient words for each type of toxicity really shows its own set of bad words for each type. However, zero coefficient words, especially the disguised words, seem to be similar for all types. Thus, I will use high coefficient words for the most general type 'toxicity' in order to extract bad words from concatenated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "I did some error analysis for each type of toxicity since I did this only for 'toxicity' in Part1. However, I will not show the sentences below due to profane, vulgar, or offensive words (uncommenting can show them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame()\n",
    "pred['comment_text']=train['comment_text']\n",
    "pred['comment_stemmed'] =train_comment_stemmed\n",
    "\n",
    "def error_analysis(category, prob_cut, num_comment_check):\n",
    "    clf = LogisticRegression(solver='sag', C=best_C[category],random_state =42)   \n",
    "    clf.fit(train_feature_matrix, train[category]) \n",
    "    pred[category] =train[category] #actual label\n",
    "    pred['pred_prob'] = clf.predict_proba(train_feature_matrix)[:,1] #how likely the label is 1\n",
    "    print(\"### {} ###\\n\".format(category))\n",
    "    # positive with low predicted probability\n",
    "    print(\"False Negatives:\")\n",
    "    print(pred[(pred[category]==1) & (pred['pred_prob']< prob_cut)]['comment_stemmed'].values[:num_comment_check])\n",
    "    print()\n",
    "    # negative with high predicted probability\n",
    "    print(\"False Positives:\")\n",
    "    print(pred[(pred[category]==0) & (pred['pred_prob']> (1-prob_cut))]['comment_stemmed'].values[:num_comment_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#error_analysis('toxic', 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_analysis('severe_toxic', 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#error_analysis('obscene', 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_analysis('threat', 0.2, 5) #had to increase threshold due to not ebough cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_analysis('insult', 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_analysis('identity_hate', 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think many of the false positives shoul be real positives. Some of them are containing several bad words and predicted as toxic, but human-rated as nontoxic. Thus, some false positives are unavoidable errors by human raters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'Section3'></a>\n",
    "## 3. TFfidf + Logistic regression after More Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Cleaning \n",
    "__Cleaning plans__\n",
    "\n",
    "- Change the numbers in disguised words back into letters (e.g. 5h1t --> shit)\n",
    "- Extract bad words hidden in concatenated words\n",
    "- Make too long nonsense words into 'toolongword' after extracting bad words (it might become a high coefficient word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation for removing diguised words with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaty maping numbers to letters of similar shapes\n",
    "num_to_char = {'1':'i', #e.g. '5h1t', 'c0pyr1ght'\n",
    "               '3':'g', #e.g. 'ni33er'\n",
    "               '5':'s', #e.g. '5h1t', '5hut', '5uck5'\n",
    "               '0':'o', #e.g. 'c0pyr1ght'\n",
    "               '2':'2', '4':'4','6':'6','7':'7','8':'8','9':'9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = '1sd3g5h0f6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isdggshof6\n"
     ]
    }
   ],
   "source": [
    "if re.search('((?:[a-z]+[0-9]|[0-9]+[a-z])[a-z0-9]*)',test_str):\n",
    "        print(re.sub('(\\d)', lambda m: num_to_char[m.group()], test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = re.compile(r'((?:[a-z]+[0-9]|[0-9]+[a-z])[a-z0-9]*)')\n",
    "#if pattern.search(test_str):\n",
    "#        print(re.sub('(\\d+)', lambda m: num_to_char[m.group()], test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation for removing disguised words with concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'toxic'\n",
    "clf = LogisticRegression(solver='sag', C=best_C[category],random_state =42)   \n",
    "clf.fit(train_feature_matrix, train[category])\n",
    "coeff = clf.coef_\n",
    "sorted_voca = [word for _,word in sorted(zip(coeff[0],feature_words),reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'idiot', 'stupid', 'shit', 'suck', 'bullshit', 'ass', 'asshol', 'bitch', 'dick', 'faggot', 'crap', 'peni', 'bastard', 'moron', 'pathet', 'nigger', 'fucker', 'cunt', 'jerk', 'shut', 'dumbass', 'cock', 'gay', 'motherfuck', 'hell', 'dumb', 'loser', 'retard', 'fag', 'pussi', 'fuckin', 'liar', 'racist', 'piss', 'dickhead', 'fool', 'jackass', 'nazi', 'kill', 'sex', 'cocksuck', 'hate', 'wtf', 'sick', 'pig', 'damn', 'whore', 'bloodi', 'homosexu', 'garbag', 'die', 'douchebag', 'goddamn', 'fing', 'scum', 'hypocrit', 'fk', 'wanker', 'vagina', 'ars', 'fuckhead', 'dirti', 'fck', 'fat', 'disgust', 'dipshit', 'nerd', 'pedophil', 'fart', 'prick', 'anal', 'fascist', 'rubbish', 'donkey', 'ugli', 'freak', 'screw', 'imbecil', 'rape', 'fcking', 'nigga', 'stink', 'porn', 'shame', 'ridicul', 'coward', 'shitti', 'fuk', 'looser', 'lick', 'monkey', 'fuckwit', 'foolish', 'ahol', 'homo', 'mental', 'butt', 'scumbag']\n"
     ]
    }
   ],
   "source": [
    "bad_words = [ word for word in sorted_voca[:100] if len(word)>1]\n",
    "print(bad_words) #'f' removed, so 99 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are high coefficient words for 'toxicity'. They will be extracted from concatenated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'aaaaidiotfooldumb'\n",
    "#long code\n",
    "#words_to_be_added = ''\n",
    "#for word in bad_words:   \n",
    "#    if test_word.find(word) >=0:\n",
    "#        words_to_be_added += word + \" \"\n",
    "#test_word = words_to_be_added \n",
    "#test_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idiot dumb fool'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([word for word in bad_words if test_word.find(word) >=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word2 = 'goodwordscocatenated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([word for word in bad_words if test_word2.find(word) >=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be added to stemming function\n",
    "def more_cleaning(word_stemmed):\n",
    "    ### more cleaning here ###\n",
    "    # diguised with numbers\n",
    "    if re.search('((?:[a-z]+[0-9]|[0-9]+[a-z])[a-z0-9]*)',word_stemmed):\n",
    "        word_stemmed = re.sub('(\\d)', lambda m: num_to_char[m.group()], word_stemmed)\n",
    "    # disguised with concatenation\n",
    "    hidden_bad_words = \" \".join([word for word in bad_words if word_stemmed.find(word) >=0])\n",
    "    if hidden_bad_words !='':\n",
    "        word_stemmed = hidden_bad_words\n",
    "    # too long without space (i.e. not treated in the above condition, but long)\n",
    "    if (len(word_stemmed) > 20) & (word_stemmed.find(' ') == -1):\n",
    "        word_stemmed = 'toolongword'\n",
    "    return word_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shit'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_cleaning('klajthwetsjethjkh5h1ttkshtkjthkejhksh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toolongword'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_cleaning('5378632852jsghjkdhjskghjgklshjkglhsgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_more_clean(text):\n",
    "    #remove punctuations\n",
    "    text_string = re.sub(r'[^\\w\\s]','',text)\n",
    "    #split text into words\n",
    "    word_list = text_string.split()\n",
    "    #apply stemmer and combine them again\n",
    "    stemmer= SnowballStemmer(\"english\")\n",
    "    words = \"\"\n",
    "    for word in word_list:\n",
    "        if word != \"\":\n",
    "            word_stemmed = stemmer.stem(word)\n",
    "            \n",
    "            ##### more cleaning added here #####\n",
    "            # diguised with numbers\n",
    "            if re.search('((?:[a-z]+[0-9]|[0-9]+[a-z])[a-z0-9]*)',word_stemmed):\n",
    "                word_stemmed = re.sub('(\\d)', lambda m: num_to_char[m.group()], word_stemmed)\n",
    "            # disguised with concatenation\n",
    "            hidden_bad_words = \" \".join([word for word in bad_words if word_stemmed.find(word) >=0])\n",
    "            if hidden_bad_words !='':\n",
    "                word_stemmed = hidden_bad_words\n",
    "            # too long without space (i.e. not cocaternated in the above condition, but long)\n",
    "            if (len(word_stemmed) > 20) & (word_stemmed.find(' ') == -1):\n",
    "                word_stemmed = 'toolongword'\n",
    "            ####################################\n",
    "            \n",
    "            words += word_stemmed + \" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_comment_stemmed_clean = train_comment.apply(text_preprocessing_more_clean) \n",
    "test_comment_stemmed_clean = test_comment.apply(text_preprocessing_more_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stemmed and more cleaned comments\n",
    "vectorizer = TfidfVectorizer(analyzer ='word', \n",
    "                             stop_words='english',\n",
    "                             sublinear_tf=True, #term-freq scaling\n",
    "                             strip_accents='unicode', #works generally\n",
    "                             token_pattern=r'\\w{1,}', #1+ char words\n",
    "                             ngram_range=(1,1),\n",
    "                             max_features=20000) #top frequent 20000 words\n",
    "\n",
    "vectorizer.fit(pd.concat([train_comment_stemmed_clean,test_comment_stemmed_clean]))\n",
    "# Make sure to use the right argument\n",
    "train_feature_matrix = vectorizer.transform(train_comment_stemmed_clean)\n",
    "test_feature_matrix = vectorizer.transform(test_comment_stemmed_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9658 (+/-0.0036) for {'C': 0.2}\n",
      "0.9694 (+/-0.0030) for {'C': 0.5}\n",
      "0.9709 (+/-0.0026) for {'C': 1}\n",
      "0.9711 (+/-0.0024) for {'C': 2}\n",
      "0.9696 (+/-0.0024) for {'C': 5}\n",
      "\n",
      "### severe_toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 0.5}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9871 (+/-0.0007) for {'C': 0.2}\n",
      "0.9877 (+/-0.0012) for {'C': 0.5}\n",
      "0.9876 (+/-0.0019) for {'C': 1}\n",
      "0.9869 (+/-0.0028) for {'C': 2}\n",
      "0.9847 (+/-0.0042) for {'C': 5}\n",
      "\n",
      "### obscene ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9861 (+/-0.0018) for {'C': 0.2}\n",
      "0.9874 (+/-0.0019) for {'C': 0.5}\n",
      "0.9879 (+/-0.0020) for {'C': 1}\n",
      "0.9878 (+/-0.0020) for {'C': 2}\n",
      "0.9864 (+/-0.0020) for {'C': 5}\n",
      "\n",
      "### threat ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9728 (+/-0.0029) for {'C': 0.2}\n",
      "0.9772 (+/-0.0029) for {'C': 0.5}\n",
      "0.9802 (+/-0.0035) for {'C': 1}\n",
      "0.9818 (+/-0.0040) for {'C': 2}\n",
      "0.9812 (+/-0.0046) for {'C': 5}\n",
      "\n",
      "### insult ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9760 (+/-0.0023) for {'C': 0.2}\n",
      "0.9778 (+/-0.0016) for {'C': 0.5}\n",
      "0.9783 (+/-0.0012) for {'C': 1}\n",
      "0.9779 (+/-0.0009) for {'C': 2}\n",
      "0.9756 (+/-0.0005) for {'C': 5}\n",
      "\n",
      "### identity_hate ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9753 (+/-0.0026) for {'C': 0.2}\n",
      "0.9779 (+/-0.0023) for {'C': 0.5}\n",
      "0.9788 (+/-0.0021) for {'C': 1}\n",
      "0.9788 (+/-0.0019) for {'C': 2}\n",
      "0.9770 (+/-0.0019) for {'C': 5}\n",
      "\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    parameters = {'C': [.2, .5, 1, 2, 5]}\n",
    "    log_reg = LogisticRegression(solver = 'sag',random_state =42)   \n",
    "    clf= GridSearchCV(log_reg, parameters, scoring='roc_auc', cv=3)\n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More find tuning for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.6}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9715 (+/-0.0018) for {'C': 1.1}\n",
      "0.9716 (+/-0.0018) for {'C': 1.2}\n",
      "0.9716 (+/-0.0018) for {'C': 1.3}\n",
      "0.9717 (+/-0.0018) for {'C': 1.4}\n",
      "0.9717 (+/-0.0018) for {'C': 1.5}\n",
      "0.9717 (+/-0.0018) for {'C': 1.6}\n",
      "0.9717 (+/-0.0018) for {'C': 1.7}\n",
      "0.9717 (+/-0.0018) for {'C': 1.8}\n",
      "0.9716 (+/-0.0018) for {'C': 1.9}\n",
      "0.9716 (+/-0.0018) for {'C': 2.0}\n",
      "0.9716 (+/-0.0018) for {'C': 2.1}\n",
      "0.9715 (+/-0.0018) for {'C': 2.2}\n",
      "0.9715 (+/-0.0018) for {'C': 2.3}\n",
      "0.9715 (+/-0.0018) for {'C': 2.4}\n",
      "0.9714 (+/-0.0018) for {'C': 2.5}\n",
      "\n",
      "### severe_toxic ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 0.6}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9866 (+/-0.0018) for {'C': 0.1}\n",
      "0.9873 (+/-0.0014) for {'C': 0.2}\n",
      "0.9876 (+/-0.0013) for {'C': 0.3}\n",
      "0.9877 (+/-0.0014) for {'C': 0.4}\n",
      "0.9878 (+/-0.0015) for {'C': 0.5}\n",
      "0.9878 (+/-0.0016) for {'C': 0.6}\n",
      "0.9878 (+/-0.0017) for {'C': 0.7}\n",
      "0.9877 (+/-0.0019) for {'C': 0.8}\n",
      "0.9877 (+/-0.0020) for {'C': 0.9}\n",
      "0.9876 (+/-0.0021) for {'C': 1.0}\n",
      "\n",
      "### obscene ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9879 (+/-0.0025) for {'C': 0.6}\n",
      "0.9880 (+/-0.0025) for {'C': 0.7}\n",
      "0.9881 (+/-0.0025) for {'C': 0.8}\n",
      "0.9882 (+/-0.0025) for {'C': 0.9}\n",
      "0.9882 (+/-0.0025) for {'C': 1.0}\n",
      "0.9882 (+/-0.0025) for {'C': 1.1}\n",
      "0.9882 (+/-0.0025) for {'C': 1.2}\n",
      "0.9882 (+/-0.0026) for {'C': 1.3}\n",
      "0.9882 (+/-0.0026) for {'C': 1.4}\n",
      "0.9882 (+/-0.0026) for {'C': 1.5}\n",
      "\n",
      "### threat ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 2.2}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9822 (+/-0.0079) for {'C': 1.6}\n",
      "0.9822 (+/-0.0079) for {'C': 1.7}\n",
      "0.9823 (+/-0.0079) for {'C': 1.8}\n",
      "0.9824 (+/-0.0079) for {'C': 1.9}\n",
      "0.9824 (+/-0.0078) for {'C': 2.0}\n",
      "0.9824 (+/-0.0078) for {'C': 2.1}\n",
      "0.9824 (+/-0.0078) for {'C': 2.2}\n",
      "0.9824 (+/-0.0078) for {'C': 2.3}\n",
      "0.9824 (+/-0.0078) for {'C': 2.4}\n",
      "0.9824 (+/-0.0078) for {'C': 2.5}\n",
      "0.9824 (+/-0.0077) for {'C': 2.6}\n",
      "0.9824 (+/-0.0077) for {'C': 2.7}\n",
      "0.9824 (+/-0.0077) for {'C': 2.8}\n",
      "0.9824 (+/-0.0077) for {'C': 2.9}\n",
      "0.9823 (+/-0.0077) for {'C': 3.0}\n",
      "\n",
      "### insult ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.0}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9784 (+/-0.0019) for {'C': 0.6}\n",
      "0.9785 (+/-0.0020) for {'C': 0.7}\n",
      "0.9786 (+/-0.0020) for {'C': 0.8}\n",
      "0.9786 (+/-0.0020) for {'C': 0.9}\n",
      "0.9786 (+/-0.0021) for {'C': 1.0}\n",
      "0.9786 (+/-0.0021) for {'C': 1.1}\n",
      "0.9786 (+/-0.0022) for {'C': 1.2}\n",
      "0.9786 (+/-0.0022) for {'C': 1.3}\n",
      "0.9785 (+/-0.0022) for {'C': 1.4}\n",
      "0.9785 (+/-0.0022) for {'C': 1.5}\n",
      "\n",
      "### identity_hate ###\n",
      "\n",
      "Best hyper-parameters on development set:\n",
      "{'C': 1.3}\n",
      "\n",
      "Grid search scores on development set:\n",
      "0.9788 (+/-0.0048) for {'C': 0.6}\n",
      "0.9790 (+/-0.0048) for {'C': 0.7}\n",
      "0.9792 (+/-0.0047) for {'C': 0.8}\n",
      "0.9793 (+/-0.0047) for {'C': 0.9}\n",
      "0.9793 (+/-0.0046) for {'C': 1.0}\n",
      "0.9794 (+/-0.0046) for {'C': 1.1}\n",
      "0.9794 (+/-0.0046) for {'C': 1.2}\n",
      "0.9794 (+/-0.0046) for {'C': 1.3}\n",
      "0.9794 (+/-0.0046) for {'C': 1.4}\n",
      "0.9794 (+/-0.0046) for {'C': 1.5}\n",
      "\n",
      "Wall time: 10min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune C for each category CV=5 \n",
    "parameters_dic={'toxic':{'C':[1.1,1.2,1.3,1.4,1.5, 1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5]},\n",
    "                'severe_toxic':{'C':[.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0]},\n",
    "                'obscene':{'C':[.6,.7,.8,.9,1.0,1.1,1.2,1.3,1.4,1.5]},\n",
    "                'threat':{'C':[1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0]}, \n",
    "                'insult':{'C':[.6,.7,.8,.9,1.0,1.1,1.2,1.3,1.4,1.5]}, \n",
    "                'identity_hate':{'C':[.6,.7,.8,.9,1.0,1.1,1.2,1.3,1.4,1.5]}}\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    parameters = parameters_dic[category]\n",
    "    log_reg = LogisticRegression(solver='sag', random_state =42)   \n",
    "    clf= GridSearchCV(log_reg, parameters, scoring='roc_auc', cv=5)\n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score on dev. set for toxic is 0.9720\n",
      "Mean CV score on dev. set for severe_toxic is 0.9879\n",
      "Mean CV score on dev. set for obscene is 0.9884\n",
      "Mean CV score on dev. set for threat is 0.9829\n",
      "Mean CV score on dev. set for insult is 0.9789\n",
      "Mean CV score on dev. set for identity_hate is 0.9800\n",
      "\n",
      "Mean CV score on dev. set for All categories is 0.9817\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_C = {'toxic':1.6,'severe_toxic':.6,'obscene':1.2,\n",
    "        'threat':2.2, 'insult':1.0, 'identity_hate':1.3}\n",
    "score_all = []\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    log_reg = LogisticRegression(solver='sag',random_state =42)   \n",
    "    clf= GridSearchCV(log_reg, {'C':[best_C[category]]}, scoring='roc_auc', cv=10)\n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    print(\"Mean CV score on dev. set for %s is %0.4f\" %(category,clf.best_score_))\n",
    "    score_all.append(clf.best_score_)\n",
    "print()\n",
    "print(\"Mean CV score on dev. set for All categories is %0.4f\" %np.mean(score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "\n",
    "for category in categories:\n",
    "    train_labels = train[category]\n",
    "    clf = LogisticRegression(solver='sag', C=best_C[category],random_state =42)   \n",
    "    clf.fit(train_feature_matrix, train_labels)\n",
    "    submission[category] = clf.predict_proba(test_feature_matrix)[:,1] #second column!!\n",
    "    \n",
    "submission.to_csv('submission_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9778 \n",
    "\n",
    "_This score is the best of all the methods I tried and even better than GloVe + LSTM in Part2!!! There was 12% error decrease on AUC after more cleaning!!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.252964426877442"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error decrease in percent\n",
    "(.9778-.9747)/(1-.9747)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check those words are reallly gone in zero cofficient words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of word:feature_index key value pairs\n",
    "voca_dic = vectorizer.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of words odered by feature index\n",
    "feature_words =[None]*len(voca_dic)\n",
    "for word, index in voca_dic.items():\n",
    "    feature_words[index]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High coefficient words:\n",
      " ['fuck', 'idiot', 'stupid', 'shit', 'suck', 'bitch', 'asshol', 'dick', 'bastard', 'moron', 'crap', 'peni', 'pathet', 'dumb', 'nigger', 'cunt', 'fag', 'jerk', 'shut', 'gay', 'retard', 'cock', 'fck', 'racist', 'loser', 'pussi', 'fool', 'piss', 'nazi', 'damn', 'sick', 'scum', 'kill', 'bloodi', 'fuk', 'wtf', 'hypocrit', 'sex', 'f', 'whore', 'nigga', 'douchebag', 'dirti', 'garbag', 'wanker', 'jackass', 'disgust', 'faggot', 'vagina', 'hate', 'stink', 'ugli', 'shame', 'prick', 'freak', 'die', 'donkey', 'screw', 'rubbish', 'bullshit', 'pig', 'ridicul', 'imbecil', 'liar', 'coward', 'nerd', 'pedophil', 'kiss', 'fascist', 'cum', 'hell', 'porn', 'twat', 'masturb', 'mouth', 'homo', 'monkey', 'fart', 'silli', 'slut', 'poop', 'fk', 'worst', 'arrog', 'shove', 'fggt', 'mental', 'lazi', 'ahol', 'butt', 'douch', 'prostitut', 'hole', 'smell', 'ass', 'cougar', 'ball', 'tit', 'blood', 'disgrac']\n",
      "\n",
      "Low (negative) coefficient words:\n",
      " ['thank', 'redirect', 'pleas', 'best', 'appreci', 'case', 'mention', 'continu', 'section', 'cheer', 'consensus', 'lead', 'request', 'sorri', 'articl', 'agre', 'wikiproject', 'titl', 'exampl', 'use', 'sourc', 'apolog', 'earli', 'michael', 'themselv', 'howev', 'book', '2004', 'review', 'refer', 'list', 'experi', 'season', 'propos', 'connect', 'present', 'popular', 'welcom', 'accord', 'sure', 'futur', 'movi', 'promis', 'renam', 'involv', 'februari', 'consid', 'notic', 'idea', 'reach', 'week', 'record', 'help', '2005', 'type', 'email', 'point', 'issu', 'process', 'publish', 'definit', 'long', 'elimin', 'young', 'influenc', 'believ', 'movement', 'latin', 'descript', 'regard', 'ive', 'laurent', 'april', 'good', 'french', 'role', 'similar', 'link', 'includ', 'nomin', 'paper', 'contrib', 'convinc', 'identifi', 'updat', 'rfc', 'paragraph', 'treatment', 'januari', 'great', 'armenian', 'question', 'disput', 'dure', 'relat', 'number', 'recommend', 'fought', 'talk', 'institut']\n",
      "\n",
      "Close to zero or zero coefficient words:\n",
      " ['19025427492', '4g9enter', '8hi', '90876590876', 'aaaaaaaaaaaaaaaa', 'aad', 'abstimmungsgegn', 'abujihad', 'adalah', 'adami2ig', 'againdo', 'againsh', 'ahhhhhhhhhhhha', 'ailələrinə', 'algeri', 'allumungi', 'anderen', 'answershi', 'aoo', 'apparitionii', 'arrs', 'arrsephuck', 'asdf', 'atheismus', 'aulad', 'aur', 'ayden', 'aysen', 'azzholeseat', 'badan', 'badi', 'balalarina', 'ballsarod', 'ballsbal', 'barbercallum', 'basmati', 'beesha', 'behen', 'bgcolor', 'bgcolordarkr', 'bgcolordfdfdf', 'bgcolorlightgray', 'bila', 'bili', 'bitt', 'blaa', 'blaaam', 'blooodi', 'boabi', 'bola', 'bonergasm', 'boobsashton', 'boolloox', 'booobsashtonsimmon', 'border8px', 'brianwash', 'bstrds', 'bude', 'bytyn', 'cak', 'cawk', 'chhetri', 'chhetripadam', 'childrenpop', 'chodethi', 'clanak', 'clanku', 'congeal', 'cua', 'culet', 'cumfil', 'cοck', 'daca', 'daiki', 'dakii22', 'dalam', 'dann', 'darna', 'degmada', 'dein', 'derpderp', 'deuschebag', 'dhe', 'digunakan', 'disputenam', 'disputerez', 'disputerezz', 'diturb', 'dixz', 'dobro', 'dogulasi', 'drogon', 'dutroux', 'dutrouxwathelet', 'dvnfdjv', 'dynya', 'eachotherbenon', 'edh', 'editsmoriori', 'educacao']\n"
     ]
    }
   ],
   "source": [
    "coeff_analysis('toxic',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close to zero or zero coefficient words now show those hidden bad words are gone!\n",
    "\n",
    "This part showed that taking care of hidden bad words can improve toxic comment classifications. I believe this way of cleaning can also make the deep learning models in Part2 give enhanced results. Moreover, I checked only some parts of words, so there is still room for improvement if more problematic patterns are found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
