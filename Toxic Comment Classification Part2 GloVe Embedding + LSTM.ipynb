{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "\n",
    "## Part2: GloVe + LSTM in Keras (in progress)\n",
    "Global Vectors (GloVe)  and Long Short Term Memory (LSTM) in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the second part of Toxic comment classification project. The first part, Part1: Tfidf + Logistic Regression, is the other notebook in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import codecs, sys, os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input,Embedding,Bidirectional,LSTM,GlobalMaxPool1D,Dense,Dropout,Activation\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "train = pd.read_csv('train.csv') #training set\n",
    "test = pd.read_csv('test.csv') #test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-trained word vectors, GloVe \n",
    "## file is from https://nlp.stanford.edu/projects/glove/\n",
    "word_vector_file = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comment_array = train['comment_text'].values\n",
    "test_comment_array = test['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000 # top words to be used (I might increase this number)\n",
    "text_len = 100 # max number of words to be used in each comment\n",
    "word_vec_dim =50 # dimension of Glove vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train_comment_array)+list(test_comment_array)) #input: list of texts to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394787"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary of word to index (index for this particular data)\n",
    "word_to_index = tokenizer.word_index\n",
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('to', 2),\n",
       " ('of', 3),\n",
       " ('a', 4),\n",
       " ('and', 5),\n",
       " ('you', 6),\n",
       " ('i', 7),\n",
       " ('is', 8),\n",
       " ('that', 9),\n",
       " ('in', 10)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_index.items())[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(list(word_to_index.values())) # index starts from 1 (not 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('build', 1992),\n",
       " ('ps', 1993),\n",
       " ('worry', 1994),\n",
       " ('corrected', 1995),\n",
       " ('wife', 1996),\n",
       " ('benefit', 1997),\n",
       " ('remains', 1998),\n",
       " ('liberal', 1999),\n",
       " ('network', 2000)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_index.items())[1991:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('explanation', 3095),\n",
       " ('why', 31804),\n",
       " ('the', 917801),\n",
       " ('edits', 16189),\n",
       " ('made', 17181),\n",
       " ('under', 12228),\n",
       " ('my', 78385),\n",
       " ('username', 3172),\n",
       " ('hardcore', 320),\n",
       " ('metallica', 91)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_count = tokenizer.word_counts\n",
    "list(word_to_count.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of texts to list of index sequences, one per text\n",
    "train_comment_seq = tokenizer.texts_to_sequences(train_comment_array)\n",
    "test_comment_seq = tokenizer.texts_to_sequences(test_comment_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate and pad zeros to make equal size comments\n",
    "## Try padding='pre', truncating='pre' as well?\n",
    "train_comment_seq_pad = pad_sequences(train_comment_seq, maxlen = text_len, padding='post', truncating='post' )\n",
    "test_comment_seq_pad = pad_sequences(test_comment_seq, maxlen = text_len, padding='post', truncating='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that changes tuples to first, array of the rest \n",
    "## https://www.python-course.eu/python3_passing_arguments.php\n",
    "def get_word_vec(word,*vec): \n",
    "    return word, np.asarray(vec, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of word: GloVe_vector\n",
    "## encoding=\"utf8\" removed the error I got \n",
    "word_to_vec = dict(get_word_vec(*item.strip().split()) for item in open(word_vector_file, encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(word_to_vec.values()).mean(), np.stack(word_to_vec.values()).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embedding matrix\n",
    "## Initialize embedding matrix as a numpy array of shape (max_features, word_vec_dim)\n",
    "## (assuming max_feaures <= unique number of words in texts i.e., len(word_to_index))\n",
    "## with random numbers with mean and std of word vectors for words not in pretrained word vectors \n",
    "## words will be ordered as in the word_to_index from out texts\n",
    "embed_matrix = np.random.normal(np.stack(word_to_vec.values()).mean(), \n",
    "                                np.stack(word_to_vec.values()).std(),\n",
    "                               (max_features, word_vec_dim))\n",
    "\n",
    "for word, idx in word_to_index.items():\n",
    "    if idx > max_features: # index starts from 1 (not 0) in word_to_index\n",
    "        break\n",
    "    vec = word_to_vec.get(word, None) # need get() to get values for the case word in not in keys\n",
    "    if vec is not None:\n",
    "        embed_matrix[idx-1] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train[categories].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional LSTM(50)+dropout(0.1) GlobalMaxPool1D Dense(50 & 6)\n",
    "comment_sequences = Input(shape=(text_len,))\n",
    "X = Embedding(max_features, word_vec_dim, weights=[embed_matrix])(comment_sequences)\n",
    "X = Bidirectional(LSTM(50, return_sequences= True, dropout= 0.1, recurrent_dropout= 0.1))(X)\n",
    "X = GlobalMaxPool1D()(X)\n",
    "X = Dense(50, activation= \"relu\")(X)\n",
    "X = Dropout(0.1)(X)\n",
    "X = Dense(6, activation= \"sigmoid\")(X)\n",
    "model = Model(inputs= comment_sequences, outputs= X)\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 328s 2ms/step - loss: 0.0635 - acc: 0.9783 - val_loss: 0.0502 - val_acc: 0.9820\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 327s 2ms/step - loss: 0.0459 - acc: 0.9830 - val_loss: 0.0502 - val_acc: 0.9817\n",
      "Wall time: 10min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136939b2ba8>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_comment_seq_pad, labels, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 39s 257us/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set\n",
    "prob_predictions = model.predict([test_comment_seq_pad], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[categories]= prob_predictions #can enter multiple columns at once if columns are already there\n",
    "submission.to_csv('submission_LSTM1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9704 (worse than Tfidf + Logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 244s 2ms/step - loss: 0.1043 - acc: 0.9692 - val_loss: 0.0895 - val_acc: 0.9727\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 240s 2ms/step - loss: 0.0779 - acc: 0.9755 - val_loss: 0.0732 - val_acc: 0.9763\n",
      "Wall time: 8min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as model1 except for trainable = False\n",
    "comment_sequences = Input(shape=(text_len,))\n",
    "X = Embedding(max_features, word_vec_dim, weights=[embed_matrix], trainable= False)(comment_sequences)\n",
    "X = Bidirectional(LSTM(50, return_sequences= True, dropout= 0.1, recurrent_dropout= 0.1))(X)\n",
    "X = GlobalMaxPool1D()(X)\n",
    "X = Dense(50, activation= \"relu\")(X)\n",
    "X = Dropout(0.1)(X)\n",
    "X = Dense(6, activation= \"sigmoid\")(X)\n",
    "model = Model(inputs= comment_sequences, outputs= X)\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(train_comment_seq_pad, labels, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 50s 328us/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set\n",
    "prob_predictions = model.predict([test_comment_seq_pad], batch_size=1024, verbose=1)\n",
    "\n",
    "# submission\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[categories]= prob_predictions #can enter multiple columns at once if columns are already there\n",
    "submission.to_csv('submission_LSTM4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9211 (much worse than the trainable embedding layer!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 521s 4ms/step - loss: 0.1004 - acc: 0.9709 - val_loss: 0.0567 - val_acc: 0.9804\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 526s 4ms/step - loss: 0.0529 - acc: 0.9815 - val_loss: 0.0525 - val_acc: 0.9810\n",
      "Wall time: 17min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# not bidirectional LSTM(128)+dropout(0.5) twice \n",
    "comment_sequences = Input(shape=(text_len,))\n",
    "X = Embedding(max_features, word_vec_dim, weights=[embed_matrix])(comment_sequences)\n",
    "X = LSTM(128, return_sequences= True)(X)\n",
    "X = Dropout(rate=.5)(X)\n",
    "X = LSTM(128, return_sequences = False)(X)\n",
    "X = Dropout(rate=.5)(X)\n",
    "X = Dense(6, activation= \"sigmoid\")(X)\n",
    "\n",
    "model = Model(inputs= comment_sequences, outputs= X)\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(train_comment_seq_pad, labels, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 95s 619us/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set\n",
    "prob_predictions = model.predict([test_comment_seq_pad], batch_size=1024, verbose=1)\n",
    "\n",
    "# submission\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[categories]= prob_predictions #can enter multiple columns at once if columns are already there\n",
    "submission.to_csv('submission_LSTM3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9564 (worse than model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Ensemble: Logistic Regression with Tfidf & LSTM with GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try a very simple ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_LR = pd.read_csv('submission.csv') #AUC=.9745 (from Part1 notebook)\n",
    "pred_LSTM = pd.read_csv('submission_LSTM1.csv') #AUC=.9704 (model1 in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Emsemble = (pred_LR[categories].values + pred_LR[categories].values)/2\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[categories]= pred_Emsemble #can enter multiple columns at once if columns are already there\n",
    "submission.to_csv('submission_LR_LSTM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9744 (not better than Logistic Regression with Tfidf only) \n",
    "\n",
    "It is possible that the two model predictions are high correlated and that's why this ensemble does not perform better than the logistic regression model. Let me check the correlations for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between two model predictions:\n",
      "toxic\n",
      "0.9149\n",
      "severe_toxic\n",
      "0.7793\n",
      "obscene\n",
      "0.9185\n",
      "threat\n",
      "0.2988\n",
      "insult\n",
      "0.8697\n",
      "identity_hate\n",
      "0.6699\n"
     ]
    }
   ],
   "source": [
    "# check correlation between the two model predictions\n",
    "print('Correlation between two model predictions:') \n",
    "correl = []\n",
    "for category in categories:\n",
    "    corr = np.corrcoef(pred_LR[category], pred_LSTM[category])[0,1]\n",
    "    correl.append(corr)\n",
    "    print(category)\n",
    "    print(\"%0.4f\"%corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficients between two model predictions are pretty high except for threat (less than .3). In particular, the correlations for each of toxic, obsecene, and insult are very high (over .85). This remids me of that I tuned the hyperparameters for the logistic regression model for each category. There I found less regularization is required for more unbalanced categories. The above correlations seem to correlated with the level of imbalances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09584448, 0.00999555, 0.05294822, 0.00299553, 0.04936361,\n",
       "       0.00880486])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportions of toxic comments for each category\n",
    "positive_rate = labels.mean(axis=0)\n",
    "positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.942857142857143, pvalue=0.004804664723032055)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank order correlation between two model correlations and proportions of toxic comments\n",
    "import scipy.stats as stats\n",
    "stats.spearmanr(correl, positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This high correlation could be from poor performances of LSTM for some categories. Thus, I will now check performances of the LSTM model separately for each category. Then, I will apply different levels of regularization or even different architectures for different categories to improve the overall performance.\n",
    "\n",
    "First, I will fit the model for each category to see if this improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Fitting for toxic ###\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 342s 2ms/step - loss: 0.1509 - acc: 0.9480 - val_loss: 0.1057 - val_acc: 0.9615\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 337s 2ms/step - loss: 0.0937 - acc: 0.9651 - val_loss: 0.0987 - val_acc: 0.9641\n",
      "153164/153164 [==============================] - 54s 351us/step\n",
      "### Fitting for severe_toxic ###\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 341s 2ms/step - loss: 0.0318 - acc: 0.9900 - val_loss: 0.0217 - val_acc: 0.9910\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 337s 2ms/step - loss: 0.0216 - acc: 0.9909 - val_loss: 0.0208 - val_acc: 0.9914\n",
      "153164/153164 [==============================] - 54s 354us/step\n",
      "### Fitting for obscene ###\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 344s 2ms/step - loss: 0.0873 - acc: 0.9727 - val_loss: 0.0628 - val_acc: 0.9796\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 342s 2ms/step - loss: 0.0486 - acc: 0.9820 - val_loss: 0.0556 - val_acc: 0.9799\n",
      "153164/153164 [==============================] - 57s 372us/step\n",
      "### Fitting for threat ###\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 351s 2ms/step - loss: 0.0159 - acc: 0.9970 - val_loss: 0.0097 - val_acc: 0.9969\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 344s 2ms/step - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0084 - val_acc: 0.9969\n",
      "153164/153164 [==============================] - 58s 381us/step\n",
      "### Fitting for insult ###\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 348s 2ms/step - loss: 0.0959 - acc: 0.9678 - val_loss: 0.0716 - val_acc: 0.9726\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 341s 2ms/step - loss: 0.0614 - acc: 0.9759 - val_loss: 0.0686 - val_acc: 0.9723\n",
      "153164/153164 [==============================] - 61s 401us/step\n",
      "### Fitting for identity_hate ###\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 353s 2ms/step - loss: 0.0360 - acc: 0.9912 - val_loss: 0.0263 - val_acc: 0.9914\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 342s 2ms/step - loss: 0.0210 - acc: 0.9927 - val_loss: 0.0268 - val_acc: 0.9920\n",
      "153164/153164 [==============================] - 61s 396us/step\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "for category in categories:\n",
    "    comment_sequences = Input(shape=(text_len,))\n",
    "    X = Embedding(max_features, word_vec_dim, weights=[embed_matrix])(comment_sequences)\n",
    "    X = Bidirectional(LSTM(50, return_sequences= True, dropout= 0.1, recurrent_dropout= 0.1))(X)\n",
    "    X = GlobalMaxPool1D()(X)\n",
    "    X = Dense(50, activation= \"relu\")(X)\n",
    "    X = Dropout(0.1)(X)\n",
    "    X = Dense(1, activation= \"sigmoid\")(X)\n",
    "    model = Model(inputs= comment_sequences, outputs= X)\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])  \n",
    "    print(\"### Fitting for {} ###\".format(category))\n",
    "    model.fit(train_comment_seq_pad, train[category], batch_size=32, epochs=2, validation_split=0.1)\n",
    "    submission[category] = model.predict([test_comment_seq_pad], batch_size=1024, verbose=1)\n",
    "    \n",
    "submission.to_csv('submission_LSTM5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: .9755\n",
    "\n",
    "This is better than all of the previous models I tried including LSTM model1 above and logistic regression model in Part1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9938879274682488"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if auc for the last category identity hate is also that high\n",
    "from sklearn import metrics\n",
    "pred = model.predict([train_comment_seq_pad], batch_size=1024, verbose=1) #for identity hate\n",
    "y = train['identity_hate']\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "metrics.auc(fpr, tpr)\n",
    "# Yes, AUC is very high just like accuracy for identity hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the Glove+LSMT model is especially better than Tfidf+LR model for more severely unbalanced categories (accuracy > .99). Thus, I will use the predictions by Tfidf+LR for toxic, obscene, and insult categories and predictions by GloVe+LSMT for severe toxic, threat, and identity hate categories and see if this combined prediction improves AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_LR = pd.read_csv('submission.csv') #AUC=.9745 (from Part1 notebook)\n",
    "pred_LSTM = pd.read_csv('submission_LSTM5.csv') #AUC=.9755 \n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['toxic']= pred_LR['toxic']\n",
    "submission['severe_toxic']= pred_LSTM['severe_toxic']\n",
    "submission['obscene']= pred_LR['obscene']\n",
    "submission['threat']= pred_LSTM['threat']\n",
    "submission['insult']= pred_LR['insult']\n",
    "submission['identity_hate']= pred_LSTM['identity_hate']\n",
    "\n",
    "submission.to_csv('submission_LR_LSTM_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9751 (not better than LSTM only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try the simple ensemble model I tried above since the LSTM model was improved by fitting each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Emsemble = (pred_LR[categories].values + pred_LR[categories].values)/2\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[categories]= pred_Emsemble #can enter multiple columns at once if columns are already there\n",
    "submission.to_csv('submission_LR_LSTM_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9744 (not better than LSTM only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories severe toxic, threat, and identity hate are also those with lower correlations between the two models, so the simple ensemble model might work only for those categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['toxic']= pred_LR['toxic']\n",
    "submission['severe_toxic']= (pred_LR['severe_toxic']+pred_LSTM['severe_toxic'])/2\n",
    "submission['obscene']= pred_LR['obscene']\n",
    "submission['threat']= (pred_LR['threat']+pred_LSTM['threat'])/2\n",
    "submission['insult']= pred_LR['insult']\n",
    "submission['identity_hate']= (pred_LR['identity_hate']+pred_LSTM['identity_hate'])/2\n",
    "\n",
    "submission.to_csv('submission_LR_LSTM_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB AUC: 0.9770 \n",
    "\n",
    "Yes! This is the best AUC I've ever got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary so far\n",
    "\n",
    "- Made predictions using GloVe word embedding + LSTM in Keras. \n",
    "- Used multi-task learning for multiple lables (1 by 6 vector label) as mentioned in the future directions of Part1\n",
    "- LSTM models with multi-task learning was worse than the Tfidf + Logistic regression model\n",
    "- Fitting each category for the LSTM model was much slower, but made better predictions than the logistic regression\n",
    "- Found a simple ensemble method (the last one) can increase AUC even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
